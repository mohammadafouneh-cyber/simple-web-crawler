
# Simple Web Crawler  A lightweight recursive web crawler written in Python.  It extracts internal links from a given URL and recursively follows them up to a specified depth
## ðŸš€ Features
- Extracts all href links from HTML pages
- Normalizes relative URLs to absolute
- Restricts crawling to the same domain
- Avoids duplicate visits using a list
- Supports maximum depth to control recursion
- Handles HTTP errors gracefully
## ðŸ“¦ Requirements  
Python 3.7+ - 
-requests 
Install dependencies:
 bash
 pip install requests
